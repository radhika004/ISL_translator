{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3e3df1e2bb264ceea5424c9813055525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e6758f46c1141cfa22af8595b7381bb",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa5006012f934bceae00689dfb761a04",
            "value": 10
          }
        },
        "3e6758f46c1141cfa22af8595b7381bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa5006012f934bceae00689dfb761a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radhika004/ISL_translator/blob/master/book_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3e3df1e2bb264ceea5424c9813055525",
            "3e6758f46c1141cfa22af8595b7381bb",
            "aa5006012f934bceae00689dfb761a04"
          ]
        },
        "id": "YTCh1zDW6eNA",
        "outputId": "7cb2432a-91ec-4454-a485-6715ad792117"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "IntProgress(value=10)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e3df1e2bb264ceea5424c9813055525"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "widgets.IntProgress(value=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber ftfy\n",
        "\n",
        "import pdfplumber, json, re, os\n",
        "from ftfy import fix_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5KKnZoA6lhT",
        "outputId": "f8446d29-fcc6-4504-e776-3a1229a0c4b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, ftfy, pdfminer.six, pdfplumber\n",
            "Successfully installed ftfy-6.3.1 pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EDIT: path to your PDF file\n",
        "pdf_path = \"/content/Indian_Law_For_A_Common_Man.pdf\"\n",
        "\n",
        "def clean_text(s):\n",
        "    s = fix_text(s)\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    s = s.strip()\n",
        "    # remove page numbers like \"123\" on lines by themselves\n",
        "    s = re.sub(r'\\bPage\\s*\\d+\\b', '', s, flags=re.I)\n",
        "    return s\n",
        "\n",
        "paras = []\n",
        "with pdfplumber.open(pdf_path) as pdf:\n",
        "    for page in pdf.pages:\n",
        "        text = page.extract_text() or \"\"\n",
        "        text = clean_text(text)\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        for chunk in re.split(r'\\n{2,}', text):\n",
        "            chunk = chunk.strip()\n",
        "            if len(chunk) < 40:   # skip tiny junk\n",
        "                continue\n",
        "            paras.append(chunk)"
      ],
      "metadata": {
        "id": "hH3ESeG16omc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_long(para, max_len=800):\n",
        "    if len(para) <= max_len:\n",
        "        return [para]\n",
        "    sents = re.split(r'(?<=[。.!?])\\s+', para)\n",
        "    buckets, cur = [], \"\"\n",
        "    for s in sents:\n",
        "        if len(cur) + len(s) + 1 <= max_len:\n",
        "            cur = (cur + \" \" + s).strip()\n",
        "        else:\n",
        "            if cur:\n",
        "                buckets.append(cur)\n",
        "            cur = s\n",
        "    if cur:\n",
        "        buckets.append(cur)\n",
        "    return buckets\n",
        "\n",
        "final_paras = []\n",
        "for p in paras:\n",
        "    final_paras.extend(split_long(p))\n"
      ],
      "metadata": {
        "id": "fg9BL8CL7Ecx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save as jsonl\n",
        "out_path = \"corpus_paragraphs.jsonl\"\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for p in final_paras:\n",
        "        json.dump({\"paragraph\": p}, f, ensure_ascii=False)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "print(f\"Saved {len(final_paras)} paragraphs to {out_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTAGiwZJ7J3H",
        "outputId": "086d909d-f3b6-4a14-b6b7-d894bdd3b768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 450 paragraphs to corpus_paragraphs.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import json\n"
      ],
      "metadata": {
        "id": "Rn7JlG3N7KjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load model ONCE\n",
        "model_name = \"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
        "\n",
        "qg_pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Read paragraphs\n",
        "paragraphs = []\n",
        "with open(\"corpus_paragraphs.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        paragraphs.append(json.loads(line)[\"paragraph\"])\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "final_dataset = []\n",
        "\n",
        "for i in tqdm(range(0, len(paragraphs), batch_size), desc=\"Generating Questions\"):\n",
        "    batch = paragraphs[i:i+batch_size]\n",
        "\n",
        "    prompts = [f\"Generate 3 legal questions that can be answered using only the paragraph:\\n\\n{p}\" for p in batch]\n",
        "    outputs = qg_pipe(prompts, max_new_tokens=256, do_sample=True, temperature=0.7)\n",
        "\n",
        "    # Process each batch output\n",
        "    for paragraph, out in zip(batch, outputs):\n",
        "        questions = out[\"generated_text\"].split(\"\\n\")\n",
        "        questions = [q.strip(\"- \").strip() for q in questions if q.strip()]\n",
        "\n",
        "\n",
        "        ans_prompts = [\n",
        "            f\"Answer the question based ONLY on the paragraph:\\nParagraph: {paragraph}\\nQuestion: {q}\\nGenerate 2 answer variants.\"\n",
        "            for q in questions\n",
        "        ]\n",
        "\n",
        "        ans_outputs = qg_pipe(ans_prompts, max_new_tokens=256, do_sample=True, temperature=0.7)\n",
        "\n",
        "        for q, ans_out in zip(questions, ans_outputs):\n",
        "            answers = ans_out[\"generated_text\"].split(\"\\n\")\n",
        "            answers = [a.strip(\"- \").strip() for a in answers if a.strip()]\n",
        "\n",
        "            for ans in answers:\n",
        "                final_dataset.append({\"question\": q, \"answer\": ans})\n",
        "\n",
        "# Save final dataset\n",
        "with open(\"generated_qna.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(final_dataset, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"✅ QnA dataset saved with {len(final_dataset)} entries\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgTy2nHg7O-l",
        "outputId": "6ed409ab-92a8-4848-d8d3-843144b27225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Generating Questions: 100%|██████████| 57/57 [19:38<00:00, 20.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ QnA dataset saved with 450 entries\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save final dataset\n",
        "with open(\"generated_qna.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(final_dataset, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"✅ QnA dataset saved with {len(final_dataset)} entries\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vvo8Bhtx7cK5",
        "outputId": "cf31feda-da93-4674-c383-79c22c3eef57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ QnA dataset saved with 450 entries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s6I1dCVUEM-0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}